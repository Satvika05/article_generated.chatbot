# -*- coding: utf-8 -*-
"""streamlit.app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13eR75LmTQYEIpgZDDW76QCDFJBUOiKG1
"""

import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# Load models
@st.cache_resource
def load_model(model_name):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return pipeline("text-generation", model=model, tokenizer=tokenizer)

# Dictionary of models
models = {
    "GPT-J": "EleutherAI/gpt-j-6B",
    "Mistral": "mistralai/Mistral-7B-v0.1",
    "Falcon": "tiiuae/falcon-7b"
}

# Streamlit UI
st.title("üìù Article Generator Chatbot")
st.markdown("Generate articles using different open-source LLMs.")

# Sidebar for model selection
selected_model = st.sidebar.selectbox("Select a model:", list(models.keys()))

# Load the selected model
with st.spinner(f"Loading {selected_model}..."):
    generator = load_model(models[selected_model])

# Input prompt
user_prompt = st.text_area("Enter your topic or prompt:", "The future of AI in healthcare")

if st.button("Generate Article"):
    if user_prompt.strip():
        with st.spinner("Generating article..."):
            result = generator(user_prompt, max_length=300, do_sample=True, top_k=50, temperature=0.7)
            st.success("‚úÖ Article Generated:")
            st.write(result[0]['generated_text'])
    else:
        st.warning("Please enter a valid prompt to generate the article.")